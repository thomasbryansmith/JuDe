{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Mississippi Appellate Court Opinion PDFs from law.justia.com\n",
    "\n",
    "Data for the grant are collected from law.justia.com, a repository of court opinions across the United States. <br>\n",
    "Mississippi Court of Appeals opinions can be found at URLs with the following pattern: https://law.justia.com/cases/mississippi/court-of-appeals/2023/\n",
    "- /cases/ targets justia's case repository\n",
    "- /mississippi/ targets the state of Mississippi\n",
    "- /court-of-appeals/ targets the Court of Appeals in the target state\n",
    "- /2023/ narrows the cases to a single year, in this case 2023 [this is necessary to ensure all cases are located on the same page, rather than across multiple pages]\n",
    "\n",
    "When generalizing the code below to collect data from all available years, we will need a 'year' object consisting of a list of all available years. Using this object you can 'iterate' the code through all possible versions of this URL with a Python 'for loop'. \n",
    "\n",
    "Start out by testing this code to better understand how these loops operate:\n",
    "\n",
    "    years = [2022,2021,2020]\n",
    "    root_url = \"https://law.justia.com/cases/mississippi/court-of-appeals/\"\n",
    "    for year in years:\n",
    "         print(years + root_url + \"/\")\n",
    "\n",
    "## install beautifulsoup4 (and other important libraries if you need them!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import pandas as pd\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## collecting the citations for all 2023 cases\n",
    "### scrape the source code from the target web page using beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2020\n",
    "url = \"https://law.justia.com/cases/mississippi/court-of-appeals/\" + str(year) + \"/\"\n",
    "req = requests.get(url, headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"})\n",
    "soup = BeautifulSoup(req.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = soup.find_all(\"span\", {\"class\": \"pagination page\"})\n",
    "pages = [page.find_all(\"a\", href=True) for page in pages]\n",
    "pages = [page[0].get('href') if len(page) > 0 else '' for page in pages]\n",
    "pages = [page for page in pages if page]\n",
    "pages = [i for n, i in enumerate(pages) if i not in pages[:n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [url] + [\"https://law.justia.com\" + page for page in pages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reqs = [requests.get(url, headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}) for url in urls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### locate the citations in the raw html text by targeting '\\<span\\>' HTML tags with the class 'justia-citation'\n",
    "\n",
    "Then use the get_text() function from beautifulsoup4 to clean the HTML code, leaving you with just the citation text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions = soup.find_all(\"div\", {\"class\": \"has-padding-content-block-30 -zb\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citations = [opinion.find_all(\"span\", {\"class\": \"justia-citation\"}) for opinion in opinions]\n",
    "citations = [citation[0].get_text() for citation in citations]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### locate the pdf URLs by targeting the '\\<href\\>' tags with the class 'case-name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = [opinion.find_all(\"a\", {\"class\": \"case-name\"}, href=True) for opinion in opinions]\n",
    "links = ['https://law.justia.com' + link[0].get('href') if len(link) > 0 else 'No Link' for link in links]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now take all of this and loop it through all pages of opinions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citations = []\n",
    "links = []\n",
    "print(\"Parsing \" + str(len(reqs)) + \" Pages\")\n",
    "for req in reqs:\n",
    "    soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "    opinions = soup.find_all(\"div\", {\"class\": \"has-padding-content-block-30 -zb\"})\n",
    "\n",
    "    temp = [opinion.find_all(\"span\", {\"class\": \"justia-citation\"}) for opinion in opinions]\n",
    "    temp = [citation[0].get_text() for citation in temp]\n",
    "    citations = citations + temp\n",
    "\n",
    "    temp = [opinion.find_all(\"a\", {\"class\": \"case-name\"}, href=True) for opinion in opinions]\n",
    "    temp = ['https://law.justia.com' + link[0].get('href') if len(link) > 0 else 'No Link' for link in temp]\n",
    "    links = links + temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create data frame consisting of two columns: citation and URL to opinion PDF\n",
    "Drop all cases with no pdf.<br>Print the number of collected and dropped cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([{'citation': citation, 'url': link} for citation, link in zip(citations, links)])\n",
    "r2 = 'Dropped: ' + str(len(df[df['url'] == 'No Link']))\n",
    "df = df.drop(df[df['url'] == 'No Link'].index)\n",
    "r1 = 'Collected: ' + str(len(df))\n",
    "print(str(year))\n",
    "print(r1)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have scraped all of the metadata, we need to download all available PDFs for these cases. To do this, we need to individually query each case's URL and download the PDF from the associated web page (if one is available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd()) # check your current working directory with the 'os' library\n",
    "shutil.rmtree(\"..\\\\data\\\\court_opinions\\\\\" + str(year)) # delete the working directory we are about to create\n",
    "os.mkdir(\"..\\\\data\\\\court_opinions\\\\\" + str(year)) # create a working directory where you can save the PDFs, the leading '..' takes you up a level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url, citation in tqdm(zip(df.url, df.citation)):\n",
    "    req = requests.get(url, headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"})\n",
    "    soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "    link = soup.find_all(\"a\", {\"class\": \"pdf-icon pull-right has-margin-bottom-20\"}, href=True)[0]\n",
    "    link = \"https:\" + link.get('href')\n",
    "    response = requests.get(link, 'wb') \n",
    "    if response.status_code==200:\n",
    "        pdf = open(\"..\\\\data\\\\court_opinions\\\\\" + str(year) + \"\\\\\" + str(citation) + \".pdf\", \"wb\")\n",
    "        pdf.write(response.content)\n",
    "        pdf.close()\n",
    "    else: \n",
    "        print('Error: ' + \n",
    "              re.sub(\".pdf$\", \"\", str(citation)).upper() + \n",
    "              ' aborted with ' + \n",
    "              str(response.status_code) + \n",
    "              ' status')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to get ALL available years. <br> This will require that we put all of this code into a function, and iterate it through all years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "def justia_scrape(years, state, court):\n",
    "    print(\"+++ \" + str(datetime.now()) + \" +++\\n\")\n",
    "    print(\"//LAWJUSTIASCRAPER\")\n",
    "    print(\"//\"+state.upper()+\"/\"+court.upper()+\"\\n\")\n",
    "    \n",
    "    for year in years:\n",
    "        print(\"+++ \" + str(year))\n",
    "        \n",
    "        url = \"https://law.justia.com/cases/\" + state.lower() + \"/\" + court.lower() + \"/\" + str(year) + \"/\"\n",
    "        req = requests.get(url, headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"})\n",
    "        soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "        \n",
    "        pages = soup.find_all(\"span\", {\"class\": \"pagination page\"})\n",
    "        pages = [page.find_all(\"a\", href=True) for page in pages]\n",
    "        pages = [page[0].get('href') if len(page) > 0 else '' for page in pages]\n",
    "        pages = [page for page in pages if page]\n",
    "        pages = [i for n, i in enumerate(pages) if i not in pages[:n]]\n",
    "        \n",
    "        urls = [url] + [\"https://law.justia.com\" + page for page in pages]\n",
    "        reqs = [requests.get(url, headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}) for url in urls]\n",
    "        \n",
    "        print(\"Parsing: \" + str(len(reqs)) + \" page(s)\")\n",
    "\n",
    "        citations = []\n",
    "        links = []\n",
    "        for req in reqs:\n",
    "            soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "            opinions = soup.find_all(\"div\", {\"class\": \"has-padding-content-block-30 -zb\"})\n",
    "\n",
    "            temp = [opinion.find_all(\"span\", {\"class\": \"justia-citation\"}) for opinion in opinions]\n",
    "            temp = [citation[0].get_text() for citation in temp]\n",
    "            citations = citations + temp\n",
    "\n",
    "            temp = [opinion.find_all(\"a\", {\"class\": \"case-name\"}, href=True) for opinion in opinions]\n",
    "            temp = ['https://law.justia.com' + link[0].get('href') if len(link) > 0 else 'No Link' for link in temp]\n",
    "            links = links + temp\n",
    "            \n",
    "\n",
    "        df = pd.DataFrame([{'citation': citation, 'url': link} for citation, link in zip(citations, links)])\n",
    "        r2 = 'Dropping: ' + str(len(df[df['url'] == 'No Link']))\n",
    "        df = df.drop(df[df['url'] == 'No Link'].index)\n",
    "        r1 = 'Collecting: ' + str(len(df))\n",
    "\n",
    "        print(r1)\n",
    "        print(r2)\n",
    "\n",
    "        os.mkdir(\"..\\\\data\\\\court_opinions\\\\\" + str(year))\n",
    "\n",
    "        for url, citation in tqdm(zip(df.url, df.citation)):\n",
    "            req = requests.get(url, headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"})\n",
    "            soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "            link = soup.find_all(\"a\", {\"class\": \"pdf-icon pull-right has-margin-bottom-20\"}, href=True)[0]\n",
    "            link = \"https:\" + link.get('href')\n",
    "            response = requests.get(link, 'wb') \n",
    "            if response.status_code==200:\n",
    "                pdf = open(\"..\\\\data\\\\court_opinions\\\\\" + str(year) + \"\\\\\" + str(citation) + \".pdf\", \"wb\")\n",
    "                pdf.write(response.content)\n",
    "                pdf.close()\n",
    "            else: \n",
    "                print('Error: ' + \n",
    "                    re.sub(\".pdf$\", \"\", str(citation)).upper() + \n",
    "                    ' aborted with ' + \n",
    "                    str(response.status_code) + \n",
    "                    ' status')\n",
    "        print(\" \")\n",
    "                \n",
    "years = list(range(2004, 2023+1))\n",
    "state = 'mississippi' # can be replaced with a string value for any state\n",
    "court = 'court-of-appeals' # state dependent, for Mississippi this function will accept 'court-of-appeals' or 'supreme-court', refer to URLs for case law courts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justia_scrape(years=years, state=state, court=court)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
