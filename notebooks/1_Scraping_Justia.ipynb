{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Court Opinion PDFs from law.justia.com\n",
    "\n",
    "Data are collected from law.justia.com, a repository of court opinions across the United States. <br>\n",
    "Mississippi Court of Appeals opinions can be found at URLs with the following pattern: https://law.justia.com/cases/mississippi/court-of-appeals/2023/\n",
    "- /cases/ targets justia's case repository\n",
    "- /mississippi/ targets the state of Mississippi\n",
    "- /court-of-appeals/ targets the Court of Appeals in the target state\n",
    "- /2023/ narrows the cases to a single year, in this case 2023 [this is necessary to ensure all cases are located on the same page, rather than across multiple pages]\n",
    "\n",
    "When generalizing the code below to collect data from any set of courts and years we will need to 'iterate' the code.\n",
    "\n",
    "Start out by testing this code to better understand how iteration loops operate:\n",
    "\n",
    "    years = [2022,2021,2020]\n",
    "    root_url = \"https://law.justia.com/cases/mississippi/court-of-appeals/\"\n",
    "    for year in years:\n",
    "         print(years + root_url + \"/\")\n",
    "\n",
    "### install beautifulsoup4 (and other important libraries if you need them!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required packages and find current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape list of all Federal Appellate Courts, Federal District Courts, and State Appellate Courts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://law.justia.com/cases/federal\"\n",
    "\n",
    "req = requests.get(url, headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"})\n",
    "soup = BeautifulSoup(req.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Federal Court of Appeals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_coas = soup.find_all(\"ul\", {\"class\": \"indented\"})\n",
    "fed_coas = [fed_coa.find_all(\"a\", href=True) for fed_coa in fed_coas]\n",
    "fed_coas = [fed_coa.get('href') for fed_coa in fed_coas[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Federal District Courts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_dcs = soup.find_all(\"ul\", {\"class\": \"list-columns list-columns-three list-no-styles\"})\n",
    "fed_dcs = [fed_dc.find_all(\"a\", href=True) for fed_dc in fed_dcs]\n",
    "fed_dcs = [fed_dc.get('href') for fed_dc in fed_dcs[0]]\n",
    "\n",
    "for i in range(0,len(fed_dcs)):\n",
    "    url = \"https://law.justia.com\" + fed_dcs[i]\n",
    "\n",
    "    req = requests.get(url, headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"})\n",
    "    soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "\n",
    "    dcs = soup.find_all(\"div\", {\"class\": \"indented\"})\n",
    "    dcs = [dc.find_all(\"a\", href=True) for dc in dcs]\n",
    "    fed_dcs[i] = [dc.get('href') for dc in dcs[0]]\n",
    "\n",
    "fed_dcs = sum([v if isinstance(v, list) else [v] for v in fed_dcs],[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State Courts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://law.justia.com/cases\"\n",
    "\n",
    "req = requests.get(url, headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"})\n",
    "soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "\n",
    "scas = soup.find_all(\"ul\", {\"class\": \"list-columns list-columns-three list-no-styles\"})\n",
    "scas = [sca.find_all(\"a\", href=True) for sca in scas]\n",
    "scas = [sca.get('href') for sca in scas[1]]\n",
    "\n",
    "for i in range(0,len(scas)):\n",
    "    url = \"https://law.justia.com\" + scas[i]\n",
    "\n",
    "    req = requests.get(url, headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"})\n",
    "    soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "\n",
    "    sc = soup.find_all(\"div\", {\"class\": \"indented\"})\n",
    "    sc = [c.find_all(\"a\", href=True) for c in sc]\n",
    "    scas[i] = [c.get('href') for c in sc[0]]\n",
    "\n",
    "scas = sum([v if isinstance(v, list) else [v] for v in scas],[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function that can 'slugify' court URLs, converting into a directory-friendly format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slugify(value):\n",
    "    \"\"\"Converts text into a directory-friendly format. \n",
    "\n",
    "    Args:\n",
    "        value (string): input text for conversion\n",
    "\n",
    "    Returns:\n",
    "        string: converted text\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    value = re.sub('[^\\w\\s-]', '', value).strip().lower()\n",
    "    value = re.sub('[-\\s]+', '-', value)\n",
    "    value = re.sub('^_|_$','', value)\n",
    "\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the *justia_scrape* function\n",
    "\n",
    "This function collects the citations for all cases associated with a given court and year, then collects all URLs associated with a pdf copy of the court opinion. These PDF court opinions are scraped and entered into a data frame along with the citation number (serving as a unique identifier), and writes the data (annualized) into the \"./data/court_opinions/\" subdirectory of the current working directory. You will need to make sure the \"./data/court_opinions/\" subdirectory exists in your current working directory in order for this code to run. Please note that it will create one additional level of subdirectories organized into each court scraped (e.g., \"cases_federal_appellate-courts_caaf\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def justia_scrape(years, court):\n",
    "    \"\"\"Scrapes portable document format (PDF) legal opinion documents from law.justia.com.\n",
    "    The script or Jupyter JSON file that executes this function must be located in the root\n",
    "    alongside a subdirectory named \"data\", within which there must be a \"court_opinions\" \n",
    "    subdirectory. This function will create a directory structure within the \"court_opinions\" \n",
    "    subdirectory, arranging downloaded PDFs by year and court. \n",
    "\n",
    "    Args:\n",
    "        years (list): list of years for which you wish to download data.\n",
    "        court (string): target court, must match law.justia.com URL format. See the 'courts' object.\n",
    "    \"\"\"\n",
    "    print(\"+++ \" + str(datetime.now()) + \" +++\\n\")\n",
    "    print(\"//LAWJUSTIASCRAPER\")\n",
    "    print(\"//\"+court.upper()+\"\\n\")\n",
    "    \n",
    "    os.mkdir(\".\\\\data\\\\court_opinions\\\\\" +\n",
    "             re.sub(\"^_|_$\",\"\",re.sub(\"/\",\"_\",str(court))))\n",
    "    \n",
    "    for year in years:\n",
    "        print(\"+++ \" + str(year))\n",
    "        \n",
    "        url = \"https://law.justia.com\" + court + str(year) + \"/\"\n",
    "        req = requests.get(url, headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"})\n",
    "        soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "        \n",
    "        pages = soup.find_all(\"span\", {\"class\": \"pagination page\"})\n",
    "        pages = [page.find_all(\"a\", href=True) for page in pages]\n",
    "        pages = [page[0].get('href') if len(page) > 0 else '' for page in pages]\n",
    "        pages = [page for page in pages if page]\n",
    "        pages = [i for n, i in enumerate(pages) if i not in pages[:n]]\n",
    "        \n",
    "        urls = [url] + [\"https://law.justia.com\" + page for page in pages]\n",
    "        reqs = [requests.get(url, headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}) for url in urls]\n",
    "        \n",
    "        print(\"Parsing: \" + str(len(reqs)) + \" page(s)\")\n",
    "\n",
    "        citations = []\n",
    "        links = []\n",
    "        for req in reqs:\n",
    "            soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "            opinions = soup.find_all(\"div\", {\"class\": \"has-padding-content-block-30 -zb\"})\n",
    "            \n",
    "            temp = [opinion.find_all(\"a\", {\"class\": \"case-name\"}, href=True) for opinion in opinions]\n",
    "            temp = [link[0].get('href') if len(link) > 0 else 'No Link' for link in temp]\n",
    "            temp = [re.sub(\"/\",\"_\",re.sub(\"^/cases/|.html$\",\"\",citation)) for citation in temp]\n",
    "            citations = citations + temp\n",
    "\n",
    "            temp = [opinion.find_all(\"a\", {\"class\": \"case-name\"}, href=True) for opinion in opinions]\n",
    "            temp = ['https://law.justia.com' + link[0].get('href') if len(link) > 0 else 'No Link' for link in temp]\n",
    "            links = links + temp\n",
    "            \n",
    "\n",
    "        df = pd.DataFrame([{'citation': citation, 'url': link} for citation, link in zip(citations, links)])\n",
    "        r2 = 'Dropping: ' + str(len(df[df['url'] == 'No Link']))\n",
    "        df = df.drop(df[df['url'] == 'No Link'].index)\n",
    "        r1 = 'Collecting: ' + str(len(df))\n",
    "\n",
    "        print(r1)\n",
    "        print(r2)\n",
    "\n",
    "        os.mkdir(\".\\\\data\\\\court_opinions\\\\\" + \n",
    "                 re.sub(\"^_|_$\",\"\",re.sub(\"/\",\"_\",str(court))) + \"\\\\\" + \n",
    "                 str(year))\n",
    "\n",
    "        for url, citation in tqdm(zip(df.url, df.citation)):\n",
    "            req = requests.get(url, headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"})\n",
    "            soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "            link = soup.find_all(\"a\", {\"class\": \"pdf-icon pull-right has-margin-bottom-20\"}, href=True)\n",
    "            if len(link)!=0:\n",
    "                link = \"https:\" + link[0].get('href')\n",
    "                response = requests.get(link, 'wb') \n",
    "                if response.status_code==200:\n",
    "                    pdf = open(\".\\\\data\\\\court_opinions\\\\\" + \n",
    "                            re.sub(\"^_|_$\",\"\",re.sub(\"/\",\"_\",str(court))) + \"\\\\\" + \n",
    "                            str(year) + \"\\\\\" + \n",
    "                            slugify(str(citation)) + \".pdf\", \"wb\")\n",
    "                    pdf.write(response.content)\n",
    "                    pdf.close()\n",
    "                else: \n",
    "                    print('Error: ' + \n",
    "                        re.sub(\".pdf$\", \"\", str(citation)).upper() + \n",
    "                        ' aborted with ' + \n",
    "                        str(response.status_code) + \n",
    "                        ' status')\n",
    "        print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the *court_search* function\n",
    "This function searches through the list of URLs scraped earlier in this markdown (see code chunks 3-6), and identifies all courts associated with a given keyword. Our use case uses this simple function to identify courts for specific states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def court_search(text):\n",
    "    \"\"\"searches the courts object, a list of valid input strings for the justia_scrape function.\n",
    "\n",
    "    Args:\n",
    "        text (string): input search string\n",
    "\n",
    "    Returns:\n",
    "        list: all strings containing the search term\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    \n",
    "    for court in courts:\n",
    "        if text.lower() in court:\n",
    "            output.append(court)\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find all courts associated with a given keyword (e.g., state), and loop the *justia_scrape* through all of those courts\n",
    "Note that this can take hours, and you will need a stable internet connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from JuDe import slufigy, court_search, justia_scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "court_search('Louisiana')[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for court in [court_search('Louisiana')[8]]:\n",
    "    justia_scrape(list(range(2002, 2023+1)), court)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
